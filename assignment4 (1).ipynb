{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a66c55-ad80-4f41-84a3-15022afbce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 1\n",
    "    \n",
    "Clustering helps to identify patterns in data and is useful for exploratory data analysis, customer segmentation, \n",
    "anomaly detection, pattern recognition, and image segmentation. It is a powerful tool for understanding data and\n",
    "can help to reveal insights that may not be apparent through other methods of analysis.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d574d2-74fe-4750-a210-9c3f94b95147",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 2\n",
    "    \n",
    "DBSCAN is a density-based clustering algorithm that segregates data points into high-density regions separated by\n",
    "regions of low density. Unlike k-means or hierarchical clustering, which require specifying the number of clusters\n",
    "beforehand, DBSCAN automatically determines clusters based on the density of data points.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad7ae8c-8b90-4172-9d90-8c17f37b8f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 3\n",
    "    \n",
    "One technique to automatically determine the optimal ε value is described in this paper. This technique calculates \n",
    "the average distance between each point and its k nearest neighbors, where k is the MinPts value you selected. \n",
    "The average k-distances are then plotted in ascending order on a k-distance graph.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8e98f3-5009-49c5-8529-10dd5ae7d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 4\n",
    "    \n",
    "DBSCAN is a powerful clustering algorithm that can be used for outlier detection in machine learning. It works by \n",
    "finding clusters of points based on their density and labeling points that do not belong to any cluster as outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89750844-6935-4826-a647-01a895a65387",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 5\n",
    "    \n",
    "Differences between the two algorithms: DBSCAN is a density-based clustering algorithm, whereas K-Means is a centroid-based clustering algorithm.\n",
    "DBSCAN can discover clusters of arbitrary shapes, whereas K-Means assumes that the clusters are spherical.    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90375680-9360-476a-8aa9-e0fd819400cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 6\n",
    "    \n",
    "Yes, DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering can be applied to datasets with high-dimensional feature spaces. However, there are several potential challenges associated with using DBSCAN in high-dimensional spaces:\n",
    "\n",
    "1. **Curse of Dimensionality**: In high-dimensional spaces, the density of data points tends to become more uniform, making it difficult for DBSCAN to differentiate between dense and sparse regions. As a result, it may struggle to identify meaningful clusters and may classify most of the data points as noise.\n",
    "\n",
    "2. **Determining Epsilon (ε) and MinPts**: DBSCAN requires setting two parameters: epsilon (ε), which defines the radius of the neighborhood around each point, and MinPts, the minimum number of points within ε to consider a point as a core point. Choosing appropriate values for these parameters becomes more challenging in high-dimensional spaces due to the increased complexity of the data distribution.\n",
    "\n",
    "3. **Computational Complexity**: DBSCAN's computational complexity is influenced by the number of data points and the density of the dataset. In high-dimensional spaces, the number of calculations required to compute distances between points increases significantly, leading to longer computation times and increased memory requirements.\n",
    "\n",
    "4. **Interpretability of Results**: Interpreting clustering results becomes more challenging in high-dimensional spaces, as visualizing clusters beyond three dimensions is not feasible. Understanding and validating the quality of clusters may require additional techniques such as dimensionality reduction or cluster validity indices.\n",
    "\n",
    "To address these challenges when applying DBSCAN to high-dimensional datasets, several strategies can be considered:\n",
    "\n",
    "- **Feature Selection or Dimensionality Reduction**: Reduce the dimensionality of the dataset by selecting relevant features or applying techniques like Principal Component Analysis (PCA) or t-distributed Stochastic Neighbor Embedding (t-SNE) to transform the data into a lower-dimensional space while preserving as much variance as possible.\n",
    "\n",
    "- **Parameter Tuning**: Experiment with different values of epsilon (ε) and MinPts to find the optimal parameters that result in meaningful clusters. Techniques such as grid search or cross-validation can be employed to identify suitable parameter values.\n",
    "\n",
    "- **Preprocessing**: Normalize or scale the data to ensure that all features contribute equally to distance calculations. This can help mitigate the impact of features with different scales on the clustering results.\n",
    "\n",
    "- **Ensemble Methods**: Combine multiple clustering algorithms or run DBSCAN with different parameter settings to enhance the robustness of clustering results in high-dimensional spaces.\n",
    "\n",
    "By carefully addressing these challenges and considering appropriate preprocessing and parameter tuning strategies, DBSCAN can still be effectively applied to high-dimensional datasets for density-based clustering. However, it's essential to carefully evaluate the clustering results and interpret them in the context of the specific application domain.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a3550-3242-4f08-9963-174d0c0593ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 7\n",
    "    \n",
    "DBSCAN can find clusters of different shapes and sizes. But it has trouble finding clusters of different densities because it depends on a global value for its parameter Eps. \n",
    "Several methods have been proposed to tackle this problem, each method has its drawbacks.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0689e9-45fa-4871-a1c6-0769fad345a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 8\n",
    "    \n",
    "If the true cluster labels are unknown, as was the case with my data set, the model itself must be used to evaluate performance. An example of this type of evaluation is the Silhouette Coefficient.\n",
    "The Silhouette Coefficient is bounded between 1 and -1. The best value is 1, the worst is -1.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec9a181-9ddb-4209-ad27-cf9510de7320",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 9\n",
    "    \n",
    "DBSCAN and other 'unsupervised' clustering methods can be used to automatically propagate labels used by classifiers\n",
    "(a 'supervised' machine learning task) in what as known as 'semi-supervised' machine learning.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751072b8-7d03-4b7a-a270-08b10542de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 10\n",
    "    \n",
    "As a result, the points which are outside the dense regions are excluded and considered as the noisy points or\n",
    "outliers. This characteristic of the DBSCAN algorithm makes it a perfect fit for outlier detection and making \n",
    "clusters of any random shapes and sizes.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73163f-2fee-4582-8075-486fddcf7ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 11\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "class DBSCAN:\n",
    "    def __init__(self, epsilon, min_pts):\n",
    "        self.epsilon = epsilon\n",
    "        self.min_pts = min_pts\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.labels_ = np.zeros(len(X), dtype=int)  # Initialize cluster labels\n",
    "        cluster_label = 0\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            if self.labels_[i] != 0:  # Skip points already visited\n",
    "                continue\n",
    "            \n",
    "            neighbors = self._get_neighbors(X, i)\n",
    "            if len(neighbors) < self.min_pts:\n",
    "                self.labels_[i] = -1  # Mark as noise\n",
    "            else:\n",
    "                cluster_label += 1\n",
    "                self._expand_cluster(X, i, neighbors, cluster_label)\n",
    "        \n",
    "        return self.labels_\n",
    "\n",
    "    def _get_neighbors(self, X, idx):\n",
    "        distances = np.linalg.norm(X - X[idx], axis=1)\n",
    "        return np.where(distances <= self.epsilon)[0]\n",
    "\n",
    "    def _expand_cluster(self, X, idx, neighbors, cluster_label):\n",
    "        self.labels_[idx] = cluster_label\n",
    "        i = 0\n",
    "        while i < len(neighbors):\n",
    "            n_idx = neighbors[i]\n",
    "            if self.labels_[n_idx] == -1:  # Noise points become border points\n",
    "                self.labels_[n_idx] = cluster_label\n",
    "            elif self.labels_[n_idx] == 0:  # Unvisited points\n",
    "                self.labels_[n_idx] = cluster_label\n",
    "                new_neighbors = self._get_neighbors(X, n_idx)\n",
    "                if len(new_neighbors) >= self.min_pts:\n",
    "                    neighbors = np.concatenate((neighbors, new_neighbors))\n",
    "            i += 1\n",
    "\n",
    "# Sample dataset\n",
    "X = np.array([[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]])\n",
    "\n",
    "# DBSCAN parameters\n",
    "epsilon = 3\n",
    "min_pts = 2\n",
    "\n",
    "# Instantiate and fit DBSCAN\n",
    "dbscan = DBSCAN(epsilon, min_pts)\n",
    "labels = dbscan.fit(X)\n",
    "\n",
    "print(\"Cluster labels:\", labels)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
